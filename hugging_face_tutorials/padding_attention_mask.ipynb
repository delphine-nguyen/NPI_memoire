{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# But du notebook\n",
    "\n",
    "Ce notebook explique à quoi sert le padding et qu'est-ce qu'un \"attention mask\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[146, 112, 1396, 1151, 2613, 1111, 170, 20164, 10932, 2271, 7954, 1736, 1139, 2006, 1297, 119]\n",
      "[146, 4819, 1142, 119]\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sentences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this.\"\n",
    "]\n",
    "\n",
    "tokens = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
    "ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
    "\n",
    "print(ids[0])\n",
    "print(ids[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les deux phrases ci-dessus n'ont pas la même longueur. Si on essaye de créer un tensor à partir des listes d'IDs générés, on va avoir une erreur car un tensor doit être rectangulaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 16 at dim 1 (got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m inputs_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(ids)\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 16 at dim 1 (got 4)"
     ]
    }
   ],
   "source": [
    "inputs_ids = torch.tensor(ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pallier ça, on ajoute des tokens de padding pour que chaque phrase fasse la même longueur. Pour savoir quel token de padding ajouter, on peut faire `tokenizer.pad_token_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout de 0 pour le padding\n",
    "ids = [\n",
    "    [146, 112, 1396, 1151, 2613, 1111, 170, 20164, 10932, 2271, 7954, 1736, 1139, 2006, 1297, 119],\n",
    "    [146, 4819, 1142, 119,    0,    0,   0,     0,     0,    0,    0,    0,    0,    0,    0,   0]\n",
    "]\n",
    "\n",
    "# Création des tensors\n",
    "inputs_ids = torch.tensor(ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les attention layers prennnet en compte tous les tokens de la phrase. Pour être sûr d'obtenir le même résultat avec ou sans les tokens de padding, il faut préciser aux attention layers qu'il ne faut pas faire attention aux tokens de padding.\n",
    "\n",
    "Pour ça, on crée un \"attention mask\". C'est un tensor qui fait la même taille que `input_ids` dans lequel : \n",
    "\n",
    "- `0` = ne pas faire attention\n",
    "- `1` = faire attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All-in-one"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au final, ces deux étapes sont réalisées quand on précise `padding = True` : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 146, 112, 1396, 1151, 2613, 1111, 170, 20164, 10932, 2271, 7954, 1736, 1139, 2006, 1297, 119, 102], [101, 146, 4819, 1142, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(sentences, padding=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvMemoire",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "39002e7d175a3b46ac49ee1070e80e806334a8911058f9756deae5d8e0ecee95"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
